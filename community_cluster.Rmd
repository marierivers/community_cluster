---
title: "community_cluster"
author: "Marie Rivers"
date: "1/19/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Clustering
## K-means Clustering
### Load and plot the `penguins` dataset
```{r}
# load R packages
librarian::shelf(
  palmerpenguins, dplyr, DT, ggplot2, tibble, tidyverse, skimr)

# set seed for reproducible results
set.seed(42)

# load the dataset
data("penguins")

# look at documentation in RStudio
if (interactive())
  help(penguins)

# show data table
datatable(penguins)
```

```{r}
# skim the table for a summary
skim(penguins)
```
Only removed rows with NA values for bill length or bill depth, which resulted in 342 observations rather than 333 observations.
```{r}
# remove the rows with NAs
penguins <- penguins %>% 
  drop_na(bill_length_mm, bill_depth_mm)
```

```{r}
# plot bill length vs. bill depth, species naive
ggplot(
  penguins, aes(bill_length_mm, bill_depth_mm)) +
  geom_point()
```

```{r}
# plot bill length vs. bill depth, color by species
legend_pos <- theme(
  legend.position = c(0.95, 0.05),
  legend.justification = c("right", "bottom"),
  legend.box.just = "right")

ggplot(data = penguins, aes(bill_length_mm, bill_depth_mm, color = species)) +
  geom_point() +
  legend_pos
```
### Cluster `penguins` using `kmeans()`
```{r}
# cluster using kmeans
k <- 3 # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm),
  centers = k)

# show cluster results
penguins_k
```

```{r}
# compare clusters with species (which were not used to cluster)
table(penguins_k$cluster, penguins$species)
```

```{r}
# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

ggplot(penguins, aes(bill_length_mm, bill_depth_mm, color = Cluster)) +
  geom_point() +
  legend_pos
```

**Question:** Comparing the observed species plot with 3 species with the kmeans() cluster plot with 3 clusters, where does this “unsupervised” kmeans() technique (that does not use species to “fit” the model) produce similar versus different results? One or two sentences would suffice. Feel free to mention ranges of values along the axes.

The data set contains 151 Adelie observations, but the only 140 observations were classified as Adelie so at least 11 Adelie observations were misclassified. The species plot is most similar to the clusters for the Adelie penguins and different for the Chinstrap and Gentoo clusters. The species plot shows that Gentoo penguins typically have the smallest bill depth and a bill length that is similarly distributed to Chinstrap penguins, however, the cluster plot created one cluster with smaller bill length and a wide range of bill depth and another cluster that has larger bill lenghts and a wide range of bill depth. Chinstrap penguins typically have bill depth range that is similar to Adelie and a bill length range that is similar to Gentoo.

### Plot Voronoi diagram of clustered `penguins`
This form of clustering assigns points to the cluster based on nearest centroid. You can see breaks more clearly with a Voronoi diagram.
```{r}
librarian::shelf(ggvoronoi, scales)

# define bounding box for geom_voronoi()
xr <- extendrange(range(penguins$bill_length_mm), f = 0.1)
yr <- extendrange(range(penguins$bill_depth_mm), f = 0.1)
box <- tribble(
  ~bill_length_mm, ~bill_depth_mm, ~group,
  xr[1], yr[1], 1,
  xr[1], yr[2], 1,
  xr[2], yr[2], 1,
  xr[2], yr[1], 1,
  xr[1], yr[1], 1) %>% 
  data.frame()
```

```{r}
# cluster using kmeans
k <- 3 # number of clusters
penguins_k <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm),
  centers = k)
```

```{r}
# extract cluster assignment per observation
Cluster = factor(penguins_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(penguins_k$centers) %>% 
  mutate(Cluster = factor(1:k))
```

```{r}
# plot points with voronoi diagram showing nearest centroid
ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = Cluster)) +
  geom_point() +
  legend_pos +
  geom_voronoi(data = ctrs, aes(fill = Cluster), color = NA, alpha = 0.5, outline = box) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(data = ctrs, pch = 23, cex = 2, fill = "black")
```

Task: Show the Voronoi diagram for fewer (k=2) and more (k=8) clusters to see how assignment to cluster centroids work.

**k = 2**
```{r}
# cluster using kmeans
k <- 2 # number of clusters
penguins_k2 <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm),
  centers = k)

# extract cluster assignment per observation
Cluster_k2 = factor(penguins_k2$cluster)

# extract cluster centers
ctrs_k2 <- as.data.frame(penguins_k2$centers) %>% 
  mutate(Cluster_k2 = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = Cluster_k2)) +
  geom_point() +
  legend_pos +
  geom_voronoi(data = ctrs_k2, aes(fill = Cluster_k2), color = NA, alpha = 0.5, outline = box) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(data = ctrs_k2, pch = 23, cex = 2, fill = "black")
```
**k = 8**
```{r}
# cluster using kmeans
k <- 8 # number of clusters
penguins_k8 <- kmeans(
  penguins %>% 
    select(bill_length_mm, bill_depth_mm),
  centers = k)

# extract cluster assignment per observation
Cluster_k8 = factor(penguins_k8$cluster)

# extract cluster centers
ctrs_k8 <- as.data.frame(penguins_k8$centers) %>% 
  mutate(Cluster_k8 = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = Cluster_k8)) +
  geom_point() +
  legend_pos +
  geom_voronoi(data = ctrs_k8, aes(fill = Cluster_k8), color = NA, alpha = 0.5, outline = box) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  geom_point(data = ctrs_k8, pch = 23, cex = 2, fill = "black")
```
## Hierarchical Clustering
Next, cluster sites according to species composition. Use the dune dataset from the vegan R package.

### Load `dune` dataset
The dune data set includes meadow vegetation observations of 30 species from 20 sites. 
```{r}
librarian::shelf(cluster, vegan)

# load dune dataset from package vegan
data("dune")

# show documentation on dataset if interactive
if (interactive())
  help(dune)
```

**Question:** What are the rows and columns composed of in the dune data frame?

The rows correspond to each of the 20 sites. The columns correspond to 30 different species of meadow vegetation. The numbers in each cell are the number each species observed for each site.

### Calculate Ecological Distances on `sites`
```{r}
sites <- tribble(
  ~site, ~sp1, ~sp2, ~sp3,
  "A", 1, 1, 0,
  "B", 5, 5, 0,
  "C", 0, 0, 1) %>% 
  column_to_rownames("site")
sites
```

```{r}
sites_manhattan <- vegdist(sites, method = "manhattan")
sites_manhattan
```

```{r}
sites_euclidean <- vegdist(sites, method = "euclidean")
sites_euclidean
```

```{r}
sites_bray <- vegdist(sites, method = "bray")
sites_bray
```

### Bray-Curtis Dissimilarity on `sites`

```{r}
C_AB = 1 + 1
C_AC = 0
C_BC = 0

S_A = 1 + 1 + 0
S_B = 5 + 5 + 0
S_C = 0 + 0 + 1

B_AB = 1 - ((2 * C_AB) / (S_A + S_B) )
B_AC = 1 - ((2 * C_AC) / (S_A + S_C) )
B_BC = 1 - ((2 * C_BC) / (S_B + S_C) )
```

**Question:** In your own words, how does Bray Curtis differ from Euclidean distance? See `sites_euclidean` versus `sites_bray` from lab code, slides from Lecture 05. Clustering and reading Chapter 8 of Kindt and Coe (2005).

Ecological distance refers to methods for calculating the difference between the environments of two sites as a single statistic. 

The Bray Curtis distance metric focuses on species counts, particularly the sum of the lowest counts of shared species between two sites compared to the total cound of all species. For Bray Curtis, a dissimilarity value of 1 means that two sites have no species in common and are therefore completely disimilar. A Bray Curtis values of 0 means that two sites are identical.

Unlike Bray Curtis, the euclidean distance is not limited to values between 0 and 1. The euclidean distance is calculated using the Pythagorean theorem and a plot with each species represented on an axis and each site as a coordinate.

For euclidean dissimilarity:
d[jk] = sqrt(sum((x[ij]-x[ik])^2))
binary: sqrt(A+B-2*J)

For Bray Curtis dissimilarity:
d[jk] = (sum abs(x[ij]-x[ik]))/(sum (x[ij]+x[ik]))
binary: (A+B-2*J)/(A+B)

### Agglomerative hierarchical clustering on `dune`
```{r}
# Dissimilarity matrix
d <- vegdist(dune, method = "bray")
dim(d)

as.matrix(d)[1:5, 1:5]
```

```{r}
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete")

# Dendrogram plot of hc1
plot(hc1, cex = 0.6, hang = -1)
```
**Question:** Which function comes first, `vegdist()` or `hclust()`, and why? See HOMLR 21.3.1 Agglomerative hierarchical clustering.

For agglomerative hierarchical clustering, you calculate the dissimilarity values first using `vegdist()` and then use those values with `hclust()`. The dissimilarity structure must be calculated first because it is an input for the hierarchical clustering function `hclust()`.

```{r}
# Compute agglomerative clustering with agnes
hc2 <- agnes(dune, method = "complete")

# Agglomerative coefficient
hc2$ac
```

```{r}
# Dendrogram plot of hc2
plot(hc2, which.plots = 2)
```

```{r}
# method to assess
m <- c("average", "single", "complete", "ward")
names(m) <- c("average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(dune, method = x)$ac
}

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)
```

```{r}
# compute ward linkage clustering with agnes
hc3 <- agnes(dune, method = "ward")

# Agglomerative coefficient
hc3$ac
```

```{r}
# Dendrogram plot of hc3
plot(hc3, which.plots = 2)
```

**Question:** In your own words how does hclust() differ from agnes()? See HOMLR 21.3.1 Agglomerative hierarchical clustering and help documentation (?hclust(), ?agnes()).

Unlike `hclust()`, the `agnes()` function provides an agglomerative coefficient (AC). The AC is a measure of how much clustering structure is found. The clustering structure is more balanced when the AC value is closer to 1 and the clustering structure is less well-formed when the AC value is closer to 0. The AC values shouldn't be used to compare different sized data sets because the value of AC tends to increase as the number of observations increases.

**Question:** Of the 4 methods, which is the “best” model in terms of Agglomerative Coefficient?

Based on Agglomerative Coefficient, `ward` is the best model.

### Divisive hierarchical clustering on `dune`
```{r}
# compute divisive hierarchical clustering
hc4 <- diana(dune)

# Divise coefficient; amount of clustering structure found
hc4$dc
```

**Question:** In your own words how does agnes() differ from diana()? See HOMLR 21.3.1 Agglomerative hierarchical clustering, slides from Lecture 05. Clustering and help documentation (?agnes(), ?diana()).

Agnes and Diana are both hierarchical clustering algorithms, but the `agnes()` function is a form of agglomerative clustering that starts with each observation as its own single element cluster, then works from the bottom up to combine the two clusters that are the most similar until all observations are part of one large cluster. This method is good at identifying small clusters.

The `diana()` function is a form of divisive hierarchical clustering that starts with all observations included in one cluster, then splits that cluster into two clusters based on what what would be considered most heterogeneous until all observations are their own cluster. This method is good at identifying large clusters.

### Determining optimal clusters
```{r}
librarian::shelf(factoextra)
```

```{r}
# Plot cluster results
p1 <- fviz_nbclust(dune, FUN = hcut, method = "wss", k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(dune, FUN = hcut, method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(dune, FUN = hcut, method = "gap_stat", k.max = 10) +
  ggtitle("(C) Gap statistic")

# display plots side by side
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

**Question:** How do the optimal number of clusters compare between methods for those with a dashed line?

Based on the silhouette method, the optimal number of clusters is 4. Based on the gap statistic method, the optimal number of clusters is 3 which is smaller than predicted by teh silhouette method.

### Working with dendrograms
```{r}
# Construct dendrogram for the Ames housing example
hc5 <- hclust(d, method = "ward.D2")
dend_plot <- fviz_dend(hc5)
dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])
```

```{r}
# Ward's method
hc5 <- hclust(d, method = "ward.D2")

# Cut tree into 4 groups
k = 4
sub_grp <- cutree(hc5, k = k)

# Number of members in each cluster
table(sub_grp)
```

```{r}
# Plot full dendrogram
fviz_dend(
  hc5, 
  k = k, 
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco")
```

**Question:** In dendrogram plots, which is the biggest determinant of relatedness between observations: the distance between observations along the labeled axes or the height of their shared connection? See HOMLR 21.5 Working with dendrograms.

In dendrogram plots, the biggest determinant of relatedness between observation is the height of their shared connection.

# 2b. Community - Cluster
# Ordination
## Principal Component Analysis (PCA)

```{r}
# load R packages
librarian::shelf(dplyr, ggplot2, h2o)

# set seed for reproducible results
set.seed(42)

# get data
url <- "https://koalaverse.github.io/homlr/data/my_basket.csv"
my_basket <- readr::read_csv(url)
dim(my_basket)
```

### Performing PCA in R
```{r}
h2o.no_progress() # turn off progress bars for brevity
h2o.init(max_mem_size = "5g") # connect to H2O instance
```

```{r}
# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)
```

```{r}
# run PCA
my_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o),
  transform = "STANDARDIZE",
  impute_missing = TRUE,
  max_runtime_secs = 1000)
my_pca
```

**Question:** Why is the pca_method of “GramSVD” chosen over “GLRM”? See HOMLR 17.4 Performing PCA in R.

xxx

**Question:** How many inital principal components are chosen with respect to dimensions of the input data? See HOMLR 17.4 Performing PCA in R.

xxx

```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>% 
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point()
```

**Question:** What category of grocery items contribute most to PC1? (These are related because they're bought most often together on a given grocery trip)

xxx

```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>% 
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text()
```

**Question:** What category of grocery items contribute the least to PC1 but positively towards PC2?

xxx

### Eigenvalue criterion
```{r}
# Compute eigenvalues
eigen <- my_pca@model$importance["Standard deviation", ] %>% 
  as.vector() %>% 
  .^2

# sum of all eigenvalues equals number of variables
sum(eigen)
```

```{r}
# Find PCs where the sum of eigenvalues is greater than or equal to 1
which(eigen >= 1)
```

```{r}
# Extract PVE and CVE
ve <- data.frame(
  PC = my_pca@model$importance %>% seq_along(),
  PVE = my_pca@model$importance %>%  .[2, ] %>%  unlist(),
  CVE = my_pca@model$importance %>% .[3, ] %>%  unlist())
```

```{r}
# Plot PVE and CVE
ve %>% 
  tidyr::gather(metric, variance_explained, -PC) %>% 
  ggplot(aes(PC, variance_explained)) +
  geom_point() +
  facet_wrap(~ metric, ncol = 1, scales = "free")
```

```{r}
# How many PCs required to explain at least 75% of total variable
min(which(ve$CVE >= 0.75))
```

```{r}
# Screee plot criterion
data.frame(
  PC = my_pca@model$importance %>% seq_along,
  PVE = my_pca@model$importance %>%  .[2, ] %>%  unlist()) %>% 
  ggplot(aes(PC, PVE, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002)
```
**Question:** How many principal components would you include to explain 90% of the total variance?

xxx

**Question:** How many principal components to include up to the elbow of the PVE, i.e. the “elbow” before plateau of dimensions explaining the least variance?

xxx

**Question:** What are a couple of disadvantages to using PCA? See HOMLR 17.6 Final thoughts.

xxx

## Non-metric MultiDimensional Scaling (NMDS)
### Unconstrained Ordination on Species
```{r}
# load R packages
librarian::shelf(vegan, vegan3d)

# vegetation and environment inlichen pastures from Vare et al (1995)
data("varespec") # species
data("varechem") # chemistry

varespec %>% tibble()
```

**Question:** What are the dimensions of the varespec data frame and what do rows versus columns represent?

xxx

```{r}
vare.dis <- vegdist(varespec)
vare.mds0 <- monoMDS(vare.dis)
stressplot(vare.mds0)
```

**Question:** The “stress” in a stressplot represents the difference between the observed inpnut distance versus the fitted ordination distance. How much better is the non-metric (i.e., NMDS) fit versus a linear fit (as with PCA) in terms of \(R^2\)?

xxx

```{r}
ordiplot(vare.mds0, type = "t")
```
**Question:** What two sites are most dissimilar based on species composition for the first component MDS1? And two more most dissimilar sites for the second component MDS2?

xxx

```{r}
vare.mds <- metaMDS(varespec, trace= FALSE)
vare.mds
```

```{r}
plot(vare.mds, type = "t")
```

**Question:** What is the basic difference between metaMDS and monoMDS()? See 2.1 Non-metric Multidimensional scaling of vegantutor.pdf.

xxx

### Overlay with Environment
```{r}
ef <- envfit(vare.mds, varechem, permu = 999)
ef
```

```{r}
plot(vare.mds, display = "sites")
plot(ef, p.max = 0.05)
```

**Question:** What two soil chemistry elements have the strongest negative relationship with NMDS1 that is based on species composition?

xxx

```{r}
ef <- envfit(vare.mds ~ Al + Ca, data = varechem)
plot(vare.mds, display = "sites")
plot(ef)

tmp <- with(varechem, ordisurf(vare.mds, Al, add = TRUE))
ordisurf(vare.mds ~ Ca, data = varechem, add = TRUE, col = "green4")
```

**Question:** Which of the two NMDS axes differentiates Ca the most, i.e. has the highest value given by the contours at the end (and not middle) of the axis?

xxx

### Constrained Ordination on Species and Environment
```{r}
# ordinate on species constrained by three soil elements
vare.cca <- cca(varespec ~ Al + P + K, varechem)
vare.cca
```

```{r}
# plot ordination
plot(vare.cca)
```

```{r}
# plot 3 dimensions
ordiplot3d(vare.cca, type = "h")
```

```{r}
if (interactive()){
  ordirgl(vare.cca)
}
```

**Question:** What is the difference between “constrained” versus “unconstrained” ordination within ecological context?

xxx

**Question:** What sites are most differentiated by CCA1, i.e. furthest apart along its axis, based on species composition AND the environmnent? What is the strongest environmental vector for CCA1, i.e. longest environmental vector in the direction of the CCA1 axes?

xxx